Learning about data engineering during my internship at www.extensodata.com, Nepal.

## Week 1: 
1. I learned a lot about the linux for Data Engineers and DBA.
2. Basic and Intermediate SQL commands(joins, aggregation)
3. Advanced SQL commands(Window Functions, SQL Subqueries)

## Week 2:
1. I learned about the Hadoop ecosystem by installing Hadoop in my Ubuntu machine.
2. I installed HDFS and practiced common file operations similar to Linux.
3. I delve into the internals of mapreduce framework and HDFS by reading MapReduce paper and GFS paper.
4. I learnt about Spark by learning and installing standalone spark using docker and learnt about RDD through RDD paper.
5. I learnt about the internals of the Kafka and spark structured streaming by ingesting data to kafka topics through terminal and use spark streaming to make transformation to the kafka topics data.

## Week 3:
1. I learned alot about Apache Airflow to orchestrate the events like backup the database, triggering the DAGs.
2. One of the tasks I was given was to trigger a DAG when a file is uploaded to the minio bucket, then the zipped file is uploaded to the minio bucket and then the email is sent about these files configuration as well as file attachements.
3. I also handled then Airflow Failure of triggering a DAG and introduced the Kafka for message queue.

## Week 4:
1. I learnt about QA and QC in big Data ecosystem and wrote test cases and test plans to test the airflow DAGs.
2. I also learnt about Scrum model and used Jira.
3. I experimented with Great Expectations to handle Data Quality Testing. I learnt alot about how to manage data quality in the data engineering like schema check,etc.
4. I then went onto learn about open table format Apache Iceberg using spark and docker.
5. I also learnt how to query Apache Iceberg using Trino and then learnt about query processor and integration of Trino and Apache Iceberg to make a open lakehouse architecture.


Now I am into my 5th week and practicing ml and domain knowledge of fintech 
