Learning about data engineering during my internship at www.extensodata.com, Nepal.

## Week 1: 
1. I learned a lot about the linux for Data Engineers and DBA.
2. Basic and Intermediate SQL commands(joins, aggregation)
3. Advanced SQL commands(Window Functions, SQL Subqueries)

## Week 2:
1. I learned about the Hadoop ecosystem by installing Hadoop in my Ubuntu machine.
2. I installed HDFS and practiced common file operations similar to Linux.
3. I delve into the internals of mapreduce framework and HDFS by reading MapReduce paper and GFS paper.
4. I learnt about Spark by learning and installing standalone spark using docker and learnt about RDD through RDD paper.
5. I learnt about the internals of the Kafka and spark structured streaming by ingesting data to kafka topics through terminal and use spark streaming to make transformation to the kafka topics data.

Now I am into my 3rd week and practicing my airflow skills since I already learnt about the Airflow ecosystem before coming to the internship.
