{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AnkJU93VHXn",
        "outputId": "2f29892d-129d-4973-deda-d7579f1a513c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e6c97477ad0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://87dfbaa694b5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Writing to Multiple Sinks</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Create the Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Writing to Multiple Sinks\")\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
        "    .config('spark.jars', '/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.20.jar')\n",
        "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the kafka_df to read from kafka\n",
        "\n",
        "kafka_df = (\n",
        "    spark\n",
        "    .readStream\n",
        "    .format(\"kafka\")\n",
        "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
        "    .option(\"subscribe\", \"device-data\")\n",
        "    .option(\"startingOffsets\", \"earliest\")\n",
        "    .load()\n",
        ")"
      ],
      "metadata": {
        "id": "-sZvu8uVpVgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "1a1e5959-8e77-4c0d-e215-dbf0a92dc9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-624d1ddee470>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m kafka_df = (\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View schema for raw kafka_df\n",
        "kafka_df.printSchema()\n",
        "#kafka_df.show()\n",
        "#kafka_df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "7H3KFSiNQIMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse value from binay to string into kafka_json_df\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "fa3L2vZiQNUF",
        "outputId": "2e242564-d8d8-44c5-a6ac-a16e957cc28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'kafka_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1b7b18d83941>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkafka_json_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkafka_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cast(value as string)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'kafka_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Schema of the Pyaload\n",
        "\n",
        "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
        "\n",
        "json_schema = (\n",
        "    StructType(\n",
        "    [StructField('customerId', StringType(), True),\n",
        "    StructField('data', StructType(\n",
        "        [StructField('devices',\n",
        "                     ArrayType(StructType([\n",
        "                        StructField('deviceId', StringType(), True),\n",
        "                        StructField('measure', StringType(), True),\n",
        "                        StructField('status', StringType(), True),\n",
        "                        StructField('temperature', LongType(), True)\n",
        "                    ]), True), True)\n",
        "        ]), True),\n",
        "    StructField('eventId', StringType(), True),\n",
        "    StructField('eventOffset', LongType(), True),\n",
        "    StructField('eventPublisher', StringType(), True),\n",
        "    StructField('eventTime', StringType(), True)\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "BL7LgIszQRu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the schema to payload to read the data\n",
        "from pyspark.sql.functions import from_json,col\n",
        "\n",
        "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema)).selectExpr(\"values_json.*\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "4_51e2SGQcnA",
        "outputId": "02c4e650-a61a-49a9-f841-dc4429d28f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'kafka_json_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a6ab5efddfff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstreaming_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkafka_json_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"values_json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"values_json.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'kafka_json_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To the schema of the data, place a sample json file and change readStream to read\n",
        "streaming_df.printSchema()\n",
        "#streaming_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "ipFxEFlNQevt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets explode the data as devices contains list/array of device reading\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "cnl8welSQnZS",
        "outputId": "5c460e31-11b2-41db-c27b-f4ec57fd896d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'streaming_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4e391bdcf18a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexploded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstreaming_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_devices\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.devices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'streaming_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the schema of the exploded_df, place a sample json file and change readStream to read\n",
        "exploded_df.printSchema()\n",
        "#exploded_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "tfzDqR-xQq7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the exploded df\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "flattened_df = (\n",
        "    exploded_df\n",
        "    .drop(\"data\")\n",
        "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
        "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
        "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
        "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
        "    .drop(\"data_devices\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ekQwk8EGQ3ED",
        "outputId": "91808f6d-d005-4197-e8d8-8069ef7191e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exploded_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dcddaff8463e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m flattened_df = (\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mexploded_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deviceId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_devices.deviceId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exploded_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the schema of the flattened_df, place a sample json file and change readStream to read\n",
        "flattened_df.printSchema()\n",
        "#flattened_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "5I4DAjrQQ5MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def device_data_output(df, batch_id):\n",
        "  print('Batch ID '+ str(batch_id))\n",
        "  df.write.format('parquet').mode('append').save('/home/rohit/data/device-data')\n",
        "\n",
        "  # Write to JDBC Postgres\n",
        "  (\n",
        "      df.write\n",
        "      .mode(\"append\")\n",
        "      .format(\"jdbc\")\n",
        "      .option(\"driver\", \"org.postgresql.Driver\")\n",
        "      .option(\"url\", \"jdbc:postgresql://postgres-db-1:5432/sqlpad\")\n",
        "      .option(\"dbtable\", \"device_data\")\n",
        "      .option(\"user\", \"sqlpad\")\n",
        "      .option(\"password\", \"sqlpad\")\n",
        "      .save()\n",
        "\n",
        "  )\n",
        "  # Diplay\n",
        "  df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "eRpYPZJ-RCYc",
        "outputId": "bc0601ae-361c-4f18-8d67-32caf6ca856b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-ef9b61c35576>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ef9b61c35576>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    .write\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running foreachBatch\n",
        "# Write the output to Multiple Sinks\n",
        "(flattened_df\n",
        " .writeStream\n",
        " .foreachBatch(device_data_output)\n",
        " .trigger(processingTime='10 seconds')\n",
        " .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
        " .start()\n",
        " .awaitTermination())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "jVsIFsEeSAch",
        "outputId": "ba54ce1f-a14a-4b45-8f9c-02a7f2a56822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'flattened_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-82a352c0d7d4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running foreachBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Write the output to Multiple Sinks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m (flattened_df\n\u001b[0m\u001b[1;32m      4\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_data_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'flattened_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Txr0iYU2SSTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}